import pandas as pd
df= pd.read_csv("final_data.csv")
df



from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Features (X) are 'Air temperature | (°C)', 'Pressure | (atm)', and 'Wind speed | (m/s)'
# Target (y) is 'Power generated by system | (MW)'

# Extracting features (X) and target (y)
X = df[['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)']]
y = df['Power generated by system | (MW)']

# Splitting the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize the Random Forest regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the Random Forest regressor
rf_regressor.fit(X_train, y_train)

# Predicting on the test set
y_pred = rf_regressor.predict(X_test)

# Evaluating the model using metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("R-squared:", r2)



# Load the forecasted data for 2024
forecasted_data_2024 = pd.read_excel("wind_test_data.xlsx")

# Extracting features for prediction
X_forecasted = forecasted_data_2024[['Air temperature | (°C)', 'Pressure | (atm)', 'Wind speed | (m/s)']]

# Predict power generation using the trained Random Forest regressor
predictions = rf_regressor.predict(X_forecasted)

# Print or save the predictions
print("Predicted power generation for 2024:")
print(predictions)




actual_power_2024 = forecasted_data_2024['Power generated by system | (MW)']

# Optional: Calculate the accuracy metrics if actual data is available
mae_2024 = mean_absolute_error(actual_power_2024, predictions)
mse_2024 = mean_squared_error(actual_power_2024, predictions)
r2_2024 = r2_score(actual_power_2024, predictions)

# Optional: Print accuracy metrics
print("Mean Absolute Error for 2024:", mae_2024)
print("Mean Squared Error for 2024:", mse_2024)
print("R-squared for 2024:", r2_2024)



# # Convert the date column to datetime with the correct format

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.callbacks import EarlyStopping

# # Load the dataset
# # Load the dataset
# data = pd.read_csv("final_data.csv")

# # Manually encode stability column
# data['stability'] = data['stability'].map({'stable': 0, 'unstable': 1})

# # Split the dataset into features and target
# X = data.drop(columns=['stability','date'])
# y = data['stability']

# # Continue with model building and training

# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Standardize the features
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# # Define the ANN model
# # Build the ANN model
# model = Sequential()
# model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
# model.add(Dense(32, activation='relu'))
# model.add(Dense(1, activation='sigmoid'))  # Use a single output neuron and 'sigmoid' activation for binary classification

# # Compile the model
# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# # Train the model
# history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

# # Evaluate the model on the test set
# loss, accuracy = model.evaluate(X_test_scaled, y_test)
# print("Test Accuracy:", accuracy)






y


from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Drop unnecessary columns
data = pd.read_csv("grid_data_combined.csv")
data.isnull().sum()

data_cleaned = data.drop(columns=['date'])  # Assuming you want to drop the 'date' column

# Split the dataset into features and target
X = data_cleaned.drop(columns=['stability'])
y = data_cleaned['stability']

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Encode the categorical target variable
y_encoded = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the logistic regression model
logistic_regression_model = LogisticRegression()

# Train the model
logistic_regression_model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = logistic_regression_model.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)



from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define the parameter grid
# Define the parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'penalty': ['l1', 'l2'],  # Penalty term
    'solver': ['liblinear']  # Solver supporting l1 penalty
}


# Initialize logistic regression model
logistic_regression_model = LogisticRegression()

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)

# Fit GridSearchCV to the data
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Get the best estimator
best_model = grid_search.best_estimator_

# Predict on the test set using the best model
y_pred = best_model.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)












import pandas as pd

# Assuming 'data' contains your dataset with features and 'stability' column
# Drop the 'date' column if it's not needed for correlation analysis
data_without_date = data.drop(columns=['date'])

# Convert 'stable' and 'unstable' to numerical labels
data_without_date['stability'] = data_without_date['stability'].map({'stable': 1, 'unstable': 0})

# Compute the correlation matrix
correlation_matrix = data_without_date.corr()

# Sort the correlation values with respect to the 'stability' column
correlation_with_stability = correlation_matrix['stability'].sort_values(ascending=False)

# Print the correlation values
print(correlation_with_stability)



# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score

# # Assuming 'data' contains your dataset with features and 'stability' column
# # Select only the features p1, p2, and p3
# X = data[['p1', 'p2', 'p3']]
# y = data['stability']

# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=30)

# # Initialize the Random Forest Classifier
# random_forest_model = RandomForestClassifier()

# # Train the model
# random_forest_model.fit(X_train, y_train)

# # Predict on the test set
# y_pred = random_forest_model.predict(X_test)

# # Calculate accuracy
# accuracy = accuracy_score(y_test, y_pred)
# print("Test Accuracy:", accuracy)



df= pd.read_csv('final_data.csv')


import pandas as pd

# Step 1: Load the data
data = pd.read_csv("final_data.csv")

# Step 2: Calculate the total power generated
total_power_generated = data['p1'].sum() + data['p2'].sum() + data['p3'].sum()

# Step 3: Distribute the power to each node based on the specified percentages
power_node1 = total_power_generated * 0.20  # 20% of total power
power_node2 = total_power_generated * 0.45  # 45% of total power
power_node3 = total_power_generated * 0.35  # 35% of total power

# Step 4: Prepare a DataFrame to store the distributed power for each node
node_data = {
    'Node': ['Node 1', 'Node 2', 'Node 3'],
    'Power Generated (MW)': [power_node1, power_node2, power_node3]
}

# Create a DataFrame
power_distribution_df = pd.DataFrame(node_data)

# Calculate the total power generated including P1, P2, and P3
total_power_generated_all = total_power_generated + data['p1'].sum() + data['p2'].sum() + data['p3'].sum()

# Check if generated and distributed powers are equal
equal_powers = total_power_generated == (power_node1 + power_node2 + power_node3)

# Display the DataFrame
print(power_distribution_df)

# Display the total power generated
print("Total Power Generated (including P1, P2, P3):", total_power_generated_all, "MW")

# Display if generated and distributed powers are equal
print("Generated and Distributed Powers are Equal:", equal_powers)




